"""Backup file finder â€” discovers leftover archives, database dumps, config backups.

Technology-aware patterns, 50+ extensions, timestamp naming detection,
content-length filtering, content-type validation.
"""

from __future__ import annotations

from typing import ClassVar

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target
from basilisk.utils.batch_check import batch_head_check
from basilisk.utils.http import resolve_base_urls

# Extensions to check for each base name
BACKUP_EXTENSIONS = [
    # Archives
    ".zip", ".tar.gz", ".tgz", ".tar.bz2", ".tar", ".gz", ".rar", ".7z",
    ".cab", ".arj", ".lzh", ".z",
    # Compressed backup variants
    ".tar.xz", ".tar.zst", ".lz4",
    # Database
    ".sql", ".sql.gz", ".sql.zip", ".sql.bak", ".db", ".sqlite", ".sqlite3",
    ".dump", ".mdb", ".accdb", ".frm", ".ibd", ".MYD", ".MYI",
    # Config backups
    ".conf", ".conf.bak", ".cfg", ".cfg.bak",
    ".ini", ".ini.bak", ".yaml.bak", ".yml.bak",
    ".toml.bak", ".json.bak",
    # Editor swap / temp files
    ".swp", ".swo", ".swn", "~", ".un~",
    ".bak", ".orig", ".save", ".tmp", ".temp",
    # Version suffixes
    ".old", ".OLD", ".prev", ".1", ".2", ".3",
    ".copy", ".copy1", ".backup", ".bkp", ".bkup",
    # Log files
    ".log", ".log.1", ".log.old",
]

# Archive names that include timestamps
TIMESTAMP_PATTERNS = [
    "2020", "2021", "2022", "2023", "2024", "2025", "2026",
    "01", "02", "03", "04", "05", "06",
    "07", "08", "09", "10", "11", "12",
    # Common date formats
    "0101", "0601", "1201", "0115", "0615", "1215",
    "jan", "feb", "mar", "apr", "may", "jun",
    "jul", "aug", "sep", "oct", "nov", "dec",
    "latest", "final", "prod", "live",
]


class BackupFinderPlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="backup_finder",
        display_name="Backup File Finder",
        category=PluginCategory.PENTESTING,
        description=(
            "Discovers backup files, archives, database dumps with "
            f"{len(BACKUP_EXTENSIONS)} extensions, technology-aware patterns"
        ),
        produces=["backup_files"],
        timeout=60.0,
    )

    def _generate_paths(self, host: str, ctx=None) -> list[str]:
        """Generate backup file paths based on hostname and detected tech."""
        name = host.split(".")[0]
        domain = host.replace(".", "_")
        full_domain = host.replace(".", "_")

        extensions = BACKUP_EXTENSIONS

        prefixes = [
            name, domain, full_domain,
            "backup", "db", "database",
            "site", "www", "dump", "data", "export",
            "web", "public", "app", "src", "code",
        ]

        paths: list[str] = []

        # Base name + extension
        for prefix in prefixes:
            for ext in extensions:
                paths.append(f"{prefix}{ext}")

        # Common standalone backups
        paths.extend([
            "backup.zip", "backup.tar.gz", "backup.tgz",
            "site.zip", "site.tar.gz",
            "www.zip", "www.tar.gz",
            "public_html.zip", "public_html.tar.gz",
            "htdocs.zip", "htdocs.tar.gz",
            "wwwroot.zip",
            "db.sql", "database.sql", "dump.sql",
            "mysql.sql", "postgres.sql",
            f"{name}.sql", f"{name}.sql.gz",
            f"{name}.zip", f"{name}.tar.gz",
            "full_backup.zip", "daily_backup.zip",
            "nightly.zip", "latest.zip",
            # Config backups
            "wp-config.php.bak", "wp-config.php~",
            "wp-config.php.old", "wp-config.php.save",
            "wp-config.php.swp", "wp-config.php.orig",
            "config.php.bak", "config.php~",
            ".env.bak", ".env.old", ".env.backup",
            "web.config.bak", "web.config.old",
            "settings.py.bak", "settings.py~",
            "application.yml.bak", "application.properties.bak",
            # Framework-specific
            "wp-content/backup.zip",
            "wp-content/debug.log",
            "wp-content/uploads/backup.zip",
            "storage/logs/laravel.log",
            "storage/app/backup.zip",
            "var/log/system.log",
            "var/backups/",
            # Database dump patterns
            f"{name}_dump.sql", f"{name}_backup.sql",
            f"{name}_db.sql", f"{domain}_dump.sql",
            "mysqldump.sql", "pg_dump.sql",
            # Source code archives
            "source.zip", "src.zip", "code.zip",
            "release.zip", "deploy.zip", "dist.zip",
            "build.zip", "artifact.zip",
            # Logs
            "error.log", "debug.log", "access.log",
            "application.log", "app.log",
        ])

        # Timestamp-based names (YYYY-MM format)
        for year in ("2024", "2025", "2026"):
            for month in ("01", "06", "12"):
                for ext in (".zip", ".tar.gz", ".sql.gz"):
                    paths.append(f"backup-{year}-{month}{ext}")
                    paths.append(f"{name}-{year}-{month}{ext}")

        return list(dict.fromkeys(paths))  # Dedupe preserving order

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available",
            )

        base_urls = await resolve_base_urls(target, ctx)
        if not base_urls:
            return PluginResult.fail(
                self.meta.name, target.host,
                error="Host not reachable via HTTP(S)",
            )

        paths = self._generate_paths(target.host, ctx)
        findings: list[Finding] = []
        found: list[dict] = []

        # Content-Types that indicate real backup/archive files
        backup_content_types = (
            "application/zip", "application/x-tar", "application/gzip",
            "application/x-gzip", "application/x-rar", "application/x-7z",
            "application/octet-stream", "application/sql",
            "application/x-sqlite3", "text/x-sql", "application/x-bzip2",
        )

        for burl in base_urls:
            if ctx.should_stop:
                break
            urls = [f"{burl}/{path}" for path in paths]
            hits = await batch_head_check(
                ctx.http, urls, ctx.rate,
                concurrency=15, timeout=5.0,
                deadline=ctx._deadline,
            )

            for url, _status, size in hits:
                path = url[len(burl) + 1:] if url.startswith(burl) else url

                # Verify with GET request: check Content-Type and body signature
                content_type = ""
                verified = False
                actual_size = size
                try:
                    async with ctx.rate:
                        resp = await ctx.http.get(
                            url, timeout=5.0,
                            headers={"Range": "bytes=0-1023"},
                        )
                        content_type = resp.headers.get(
                            "Content-Type", "",
                        ).lower()
                        body_bytes = await resp.read()
                        actual_size = max(
                            size,
                            int(resp.headers.get("Content-Length", "0") or 0),
                            len(body_bytes),
                        )
                        # Signature detection
                        verified = (
                            body_bytes[:2] == b"PK"  # ZIP
                            or body_bytes[:2] == b"\x1f\x8b"  # gzip
                            or body_bytes[:5] == b"Rar!\x1a"  # RAR
                            or body_bytes[:6] == b"7z\xbc\xaf\x27\x1c"  # 7z
                            or b"SQLite format" in body_bytes[:32]
                            or any(
                                ct in content_type
                                for ct in backup_content_types
                            )
                        )
                except Exception:
                    pass

                # Accept if: large file OR verified Content-Type/signature
                if not verified and actual_size < 100:
                    continue
                # Skip HTML error pages masquerading as backups
                if "text/html" in content_type and actual_size < 5000:
                    continue

                # Classify severity based on content
                severity = "high"
                desc = "Potential backup file"

                if any(ext in path for ext in (".sql", ".db", ".sqlite", ".dump")):
                    severity = "critical"
                    desc = "Database dump/backup"
                elif any(ext in path for ext in (".zip", ".tar", ".rar", ".7z")):
                    severity = "high"
                    desc = f"Archive backup ({self._format_size(actual_size)})"
                elif "config" in path or ".env" in path:
                    severity = "critical"
                    desc = "Configuration backup"
                elif ".log" in path:
                    severity = "medium"
                    desc = "Log file"
                elif path.endswith(("~", ".swp", ".swo", ".save")):
                    severity = "high"
                    desc = "Editor swap/backup file"

                found.append({
                    "path": path, "url": url, "size": actual_size,
                    "severity": severity, "verified": verified,
                    "content_type": content_type,
                })
                findings.append(getattr(Finding, severity)(
                    f"Backup file found: {path}",
                    description=f"{desc} ({self._format_size(actual_size)})",
                    evidence=(
                        f"URL: {url}\n"
                        f"Content-Type: {content_type}\n"
                        f"Size: {self._format_size(actual_size)}"
                        f"{' [verified signature]' if verified else ''}"
                    ),
                    remediation="Remove backup files from web-accessible directories",
                    tags=["pentesting", "backup"],
                ))

        if not findings:
            findings.append(Finding.info(
                f"No backup files found ({len(paths)} paths checked)",
                tags=["pentesting", "backup"],
            ))

        return PluginResult.success(
            self.meta.name, target.host,
            findings=findings,
            data={"backup_files": found, "paths_checked": len(paths)},
        )

    @staticmethod
    def _format_size(size: int) -> str:
        for unit in ("B", "KB", "MB", "GB"):
            if size < 1024:
                return f"{size:.0f} {unit}"
            size /= 1024
        return f"{size:.1f} TB"
