"""Session token weakness detection — entropy, predictability, fixation.

Tests:
1. Session ID entropy analysis (multiple samples)
2. Sequential/predictable token detection (incremental, timestamp-based)
3. Session fixation vulnerability (pre-auth token persists post-auth)
4. Token length adequacy (OWASP minimum: 128 bits / 16 bytes)
5. Character set analysis (limited charset = reduced entropy)
"""

from __future__ import annotations

import logging
import math
import re
from collections import Counter
from typing import ClassVar

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target
from basilisk.utils.http_check import resolve_base_url

logger = logging.getLogger(__name__)

# Cookie names that typically hold session identifiers
SESSION_NAMES = {
    "phpsessid", "jsessionid", "asp.net_sessionid", "sessionid",
    "session", "sid", "sessid", "connect.sid", "laravel_session",
    "ci_session", "rack.session", "_session", "session_id",
    "dvwa", "token", "id",
}

# Normalize cookie names for matching
_NORM_RE = re.compile(r"[-_.]")

# Minimum recommended session ID length (bytes of entropy)
MIN_TOKEN_LENGTH = 16  # 128 bits
MIN_ENTROPY_PER_CHAR = 4.0  # Shannon entropy threshold
CRITICAL_ENTROPY = 2.5  # Almost certainly predictable

# Paths to request for collecting session tokens
SAMPLE_PATHS = ["/", "/login", "/index.php", "/index", "/home"]

# Number of session token samples to collect for pattern analysis
SAMPLE_COUNT = 10


class SessionCheckPlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="session_check",
        display_name="Session Token Weakness Check",
        category=PluginCategory.PENTESTING,
        description=(
            "Analyzes session token generation for predictability, "
            "low entropy, fixation, and insufficient length"
        ),
        produces=["session_findings"],
        timeout=30.0,
    )

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available",
            )

        findings: list[Finding] = []
        data: dict = {"sessions": {}, "samples": 0}

        base_url = await resolve_base_url(target.host, ctx)
        if not base_url:
            return PluginResult.success(
                self.meta.name, target.host,
                findings=[Finding.info("Host not reachable via HTTP/HTTPS")],
                data=data,
            )

        # Phase 1: Collect multiple session tokens
        session_samples = await self._collect_sessions(ctx, base_url)
        data["samples"] = sum(len(v) for v in session_samples.values())

        if not session_samples:
            findings.append(Finding.info(
                "No session cookies detected",
                tags=["pentesting", "session"],
            ))
            return PluginResult.success(
                self.meta.name, target.host, findings=findings, data=data,
            )

        # Phase 2: Analyze each session cookie
        for cookie_name, tokens in session_samples.items():
            if ctx.should_stop:
                break
            data["sessions"][cookie_name] = {
                "count": len(tokens),
                "unique": len(set(tokens)),
            }
            self._analyze_tokens(cookie_name, tokens, findings)

        # Phase 3: Session fixation test
        if not ctx.should_stop:
            await self._test_fixation(ctx, base_url, session_samples, findings, data)

        return PluginResult.success(
            self.meta.name, target.host, findings=findings, data=data,
        )

    async def _collect_sessions(
        self, ctx, base_url: str,
    ) -> dict[str, list[str]]:
        """Collect session token samples by making multiple requests."""
        sessions: dict[str, list[str]] = {}

        for i in range(SAMPLE_COUNT):
            if ctx.should_stop:
                break
            path = SAMPLE_PATHS[i % len(SAMPLE_PATHS)]
            try:
                async with ctx.rate:
                    # Fresh request without cookies to get new session
                    resp = await ctx.http.get(
                        f"{base_url}{path}", timeout=8.0,
                        headers={"Cookie": ""},
                    )
                    for cookie_str in resp.headers.getall("Set-Cookie", []):
                        name, value = self._extract_cookie(cookie_str)
                        if name and value and self._is_session_cookie(name):
                            sessions.setdefault(name, []).append(value)
            except Exception:
                continue

        return sessions

    def _analyze_tokens(
        self, cookie_name: str, tokens: list[str], findings: list[Finding],
    ) -> None:
        """Analyze a set of session tokens for weaknesses."""
        unique = list(set(tokens))

        if not unique:
            return

        # 1. Token length check
        avg_len = sum(len(t) for t in unique) / len(unique)
        if avg_len < MIN_TOKEN_LENGTH:
            findings.append(Finding.high(
                f"Short session token '{cookie_name}': avg {avg_len:.0f} chars",
                description=(
                    f"Session tokens average {avg_len:.0f} characters. "
                    f"OWASP recommends at least {MIN_TOKEN_LENGTH} characters "
                    f"(128 bits) for session identifiers."
                ),
                evidence=f"Sample: {unique[0][:40]}... (length: {len(unique[0])})",
                remediation="Use CSPRNG to generate tokens of at least 128 bits (32 hex chars)",
                tags=["pentesting", "session", "length", "owasp:a07"],
            ))

        # 2. Static / non-rotating tokens
        if len(unique) == 1 and len(tokens) >= 3:
            findings.append(Finding.critical(
                f"Static session token '{cookie_name}': same value across {len(tokens)} requests",
                description=(
                    "All requests returned the same session token. "
                    "This means either sessions are not unique per user "
                    "or the server reuses a single token."
                ),
                evidence=f"Token: {unique[0][:40]}... (seen {len(tokens)} times)",
                remediation="Generate a unique session token per session using CSPRNG",
                tags=["pentesting", "session", "static", "owasp:a07"],
            ))

        # 3. Character set analysis
        all_chars = "".join(unique)
        char_types = self._classify_charset(all_chars)
        if char_types == "numeric_only":
            findings.append(Finding.high(
                f"Session '{cookie_name}' uses only numeric characters",
                description=(
                    "Purely numeric session tokens have severely limited "
                    "keyspace and are easily brute-forced."
                ),
                evidence=f"Sample: {unique[0][:40]}",
                remediation="Use alphanumeric + special characters for session tokens",
                tags=["pentesting", "session", "charset", "owasp:a07"],
            ))
        elif char_types == "hex_only" and avg_len < 24:
            findings.append(Finding.medium(
                f"Session '{cookie_name}' uses hex charset with short length",
                description=(
                    "Hex-only tokens with less than 24 characters provide "
                    "less than 96 bits of entropy."
                ),
                evidence=f"Sample: {unique[0][:40]} (length: {len(unique[0])})",
                remediation="Increase token length or use base64/alphanumeric encoding",
                tags=["pentesting", "session", "charset"],
            ))

        # 4. Entropy analysis (per-token and aggregate)
        # Total entropy = bits/char × avg_length. Tokens need >= 128 bits total.
        if len(unique) >= 1:
            entropies = [self._shannon_entropy(t) for t in unique]
            avg_entropy = sum(entropies) / len(entropies)
            total_entropy = avg_entropy * avg_len

            if avg_entropy < CRITICAL_ENTROPY and total_entropy < 128:
                findings.append(Finding.critical(
                    f"Critically low entropy in session '{cookie_name}': "
                    f"{avg_entropy:.1f} bits/char ({total_entropy:.0f} bits total)",
                    description=(
                        "Session tokens are nearly predictable. "
                        "An attacker can enumerate valid sessions."
                    ),
                    evidence=(
                        f"Entropy: {avg_entropy:.2f} bits/char, "
                        f"total: {total_entropy:.0f} bits\n"
                        f"Samples:\n" + "\n".join(f"  {t[:50]}" for t in unique[:5])
                    ),
                    remediation="Replace token generation with CSPRNG (e.g. secrets.token_hex())",
                    tags=["pentesting", "session", "entropy", "owasp:a07"],
                ))
            elif avg_entropy < MIN_ENTROPY_PER_CHAR and total_entropy < 128:
                findings.append(Finding.high(
                    f"Low entropy session '{cookie_name}': {avg_entropy:.1f} bits/char "
                    f"({total_entropy:.0f} bits total)",
                    description="Session tokens show patterns suggesting weak randomness.",
                    evidence=(
                        f"Entropy: {avg_entropy:.2f} bits/char, "
                        f"total: {total_entropy:.0f} bits\n"
                        f"Samples:\n" + "\n".join(f"  {t[:50]}" for t in unique[:3])
                    ),
                    remediation="Use CSPRNG for session token generation",
                    tags=["pentesting", "session", "entropy", "owasp:a07"],
                ))

        # 5. Sequential / predictable pattern detection
        if len(unique) >= 3:
            self._check_sequential(cookie_name, unique, findings)

    def _check_sequential(
        self, cookie_name: str, tokens: list[str], findings: list[Finding],
    ) -> None:
        """Detect sequential or timestamp-based patterns in tokens."""
        # Try to parse as integers
        try:
            values = sorted(int(t) for t in tokens)
            if len(values) >= 3:
                diffs = [values[i + 1] - values[i] for i in range(len(values) - 1)]
                # Check for constant increment (sequential)
                if len(set(diffs)) == 1 and diffs[0] > 0:
                    findings.append(Finding.critical(
                        f"Sequential session IDs in '{cookie_name}': increment={diffs[0]}",
                        description=(
                            "Session identifiers are generated sequentially. "
                            "An attacker can predict and hijack any session."
                        ),
                        evidence=(
                            f"Tokens (sorted): {', '.join(str(v) for v in values[:5])}\n"
                            f"Constant increment: {diffs[0]}"
                        ),
                        remediation="Use cryptographically random session IDs",
                        tags=["pentesting", "session", "sequential", "owasp:a07"],
                    ))
                    return
                # Check for near-constant increment (±10% tolerance)
                avg_diff = sum(diffs) / len(diffs)
                if avg_diff > 0 and all(
                    abs(d - avg_diff) / avg_diff < 0.1 for d in diffs
                ):
                    findings.append(Finding.high(
                        f"Near-sequential session IDs in '{cookie_name}'",
                        description="Session IDs follow a predictable incrementing pattern.",
                        evidence=(
                            f"Tokens (sorted): {', '.join(str(v) for v in values[:5])}\n"
                            f"Average increment: {avg_diff:.1f}"
                        ),
                        remediation="Use cryptographically random session IDs",
                        tags=["pentesting", "session", "sequential", "owasp:a07"],
                    ))
                    return
                # Check for timestamp-based (values close to current epoch)
                import time
                now = int(time.time())
                if all(abs(v - now) < 86400 * 365 for v in values):
                    findings.append(Finding.high(
                        f"Timestamp-based session IDs in '{cookie_name}'",
                        description=(
                            "Session token values resemble Unix timestamps. "
                            "An attacker can predict tokens by guessing login time."
                        ),
                        evidence=f"Values near epoch: {', '.join(str(v) for v in values[:3])}",
                        remediation="Use CSPRNG instead of timestamps for session generation",
                        tags=["pentesting", "session", "timestamp", "owasp:a07"],
                    ))
        except (ValueError, ZeroDivisionError):
            pass

        # Check for common prefix/suffix (hash of incrementing value)
        if len(tokens) >= 3:
            prefix_len = self._common_prefix_len(tokens)
            if prefix_len > len(tokens[0]) * 0.6:
                findings.append(Finding.medium(
                    f"Session tokens share long common prefix in '{cookie_name}'",
                    description=(
                        "Session tokens share a large common prefix, suggesting "
                        "they are derived from a predictable seed."
                    ),
                    evidence=(
                        f"Common prefix ({prefix_len} chars): {tokens[0][:prefix_len]}\n"
                        f"Samples:\n" + "\n".join(f"  {t[:50]}" for t in tokens[:3])
                    ),
                    remediation="Ensure each session token is fully random",
                    tags=["pentesting", "session", "pattern"],
                ))

    async def _test_fixation(
        self, ctx, base_url: str,
        session_samples: dict[str, list[str]],
        findings: list[Finding], data: dict,
    ) -> None:
        """Test for session fixation: does the server accept a client-set session ID?"""
        for cookie_name in list(session_samples.keys())[:2]:
            fake_session = "BASILISK_FIXATION_TEST_000"
            try:
                async with ctx.rate:
                    resp = await ctx.http.get(
                        f"{base_url}/",
                        timeout=8.0,
                        headers={"Cookie": f"{cookie_name}={fake_session}"},
                    )
                    # Check if server echoed back our fake session
                    for cookie_str in resp.headers.getall("Set-Cookie", []):
                        name, value = self._extract_cookie(cookie_str)
                        if name and name.lower() == cookie_name.lower():
                            if value == fake_session:
                                findings.append(Finding.high(
                                    f"Session fixation: '{cookie_name}' accepts arbitrary values",
                                    description=(
                                        "The server accepted a client-provided session ID "
                                        "without regeneration. An attacker can fix a known "
                                        "session ID and hijack it after the victim authenticates."
                                    ),
                                    evidence=f"Sent: {cookie_name}={fake_session}\nAccepted as-is",
                                    remediation=(
                                        "Always regenerate session ID on authentication. "
                                        "Never trust client-provided session tokens."
                                    ),
                                    tags=["pentesting", "session", "fixation", "owasp:a07"],
                                ))
                                data["fixation_vulnerable"] = True
                            break
            except Exception:
                continue

    # ── Helpers ────────────────────────────────────────────────────────

    @staticmethod
    def _extract_cookie(raw: str) -> tuple[str, str]:
        """Extract (name, value) from Set-Cookie header."""
        if "=" not in raw:
            return "", ""
        parts = raw.split(";")[0]
        name, _, value = parts.partition("=")
        return name.strip(), value.strip()

    @staticmethod
    def _is_session_cookie(name: str) -> bool:
        """Check if cookie name looks like a session identifier."""
        normalized = _NORM_RE.sub("", name.lower())
        return normalized in SESSION_NAMES or "session" in normalized or "sid" in normalized

    @staticmethod
    def _shannon_entropy(data: str) -> float:
        """Shannon entropy in bits per character."""
        if not data:
            return 0.0
        counts = Counter(data)
        length = len(data)
        return -sum(
            (c / length) * math.log2(c / length)
            for c in counts.values() if c > 0
        )

    @staticmethod
    def _classify_charset(data: str) -> str:
        """Classify the character set of token values."""
        if not data:
            return "empty"
        if all(c.isdigit() for c in data):
            return "numeric_only"
        if all(c in "0123456789abcdefABCDEF" for c in data):
            return "hex_only"
        if all(c.isalnum() for c in data):
            return "alphanumeric"
        return "mixed"

    @staticmethod
    def _common_prefix_len(strings: list[str]) -> int:
        """Find length of common prefix among all strings."""
        if not strings:
            return 0
        prefix_len = 0
        for chars in zip(*strings, strict=False):
            if len(set(chars)) == 1:
                prefix_len += 1
            else:
                break
        return prefix_len
