"""Race condition detection via single-packet attack (last-byte synchronization).

Uses raw TCP connections to achieve true simultaneous request delivery:
1. Open N parallel connections, send request minus last byte
2. Barrier-sync all connections, then write final byte simultaneously
3. Analyze responses for duplicate success, limit overrun, inconsistency

Targets: stateful POST/PUT/PATCH endpoints from OpenAPI or common paths.
"""

from __future__ import annotations

import re
from collections import Counter
from typing import ClassVar
from urllib.parse import urlparse

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target

# Endpoints commonly vulnerable to race conditions
RACE_PATHS: list[dict[str, str]] = [
    {"path": "/api/coupon/apply", "method": "POST", "body": "code=DISCOUNT10"},
    {"path": "/api/transfer", "method": "POST", "body": "amount=1&to=test"},
    {"path": "/api/redeem", "method": "POST", "body": "code=GIFT123"},
    {"path": "/api/vote", "method": "POST", "body": "option=1"},
    {"path": "/api/like", "method": "POST", "body": "item=1"},
    {"path": "/api/follow", "method": "POST", "body": "user=1"},
    {"path": "/api/purchase", "method": "POST", "body": "item=1&qty=1"},
    {"path": "/api/withdraw", "method": "POST", "body": "amount=1"},
    {"path": "/api/gift", "method": "POST", "body": "to=test&amount=1"},
    {"path": "/api/bonus/claim", "method": "POST", "body": ""},
    {"path": "/api/order", "method": "POST", "body": "product=1&qty=1"},
    {"path": "/api/register", "method": "POST", "body": "email=test@test.com"},
]

# OpenAPI tags/keywords indicating stateful endpoints
STATEFUL_KEYWORDS = re.compile(
    r"(?:transfer|purchase|apply|coupon|redeem|vote|follow|like|"
    r"withdraw|gift|bonus|claim|order|payment|checkout|subscribe|"
    r"book|reserve|deposit|send|submit|approve|confirm)",
    re.IGNORECASE,
)

SUCCESS_PATTERNS = [
    re.compile(r'"success"\s*:\s*true', re.IGNORECASE),
    re.compile(r'"status"\s*:\s*"(?:ok|completed|approved|created)"', re.IGNORECASE),
    re.compile(r'"code"\s*:\s*(?:0|200|201)', re.IGNORECASE),
    re.compile(r'"result"\s*:\s*"(?:ok|success)"', re.IGNORECASE),
]

FAILURE_PATTERNS = [
    re.compile(
        r'"(?:error|already|limit|exceeded|duplicate|insufficient|expired)"',
        re.IGNORECASE,
    ),
    re.compile(r'"status"\s*:\s*"(?:fail|rejected|denied)"', re.IGNORECASE),
]

NUM_CONNECTIONS = 30


class RaceConditionPlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="race_condition",
        display_name="Race Condition (Single Packet Attack)",
        category=PluginCategory.PENTESTING,
        description=(
            "Detects race conditions via last-byte synchronization "
            "(single packet attack), response inconsistency analysis, "
            "and limit-overrun checks on stateful endpoints"
        ),
        produces=["race_condition_results"],
        timeout=90.0,
    )

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available",
            )

        from basilisk.utils.http_check import resolve_base_url
        from basilisk.utils.raw_http import (
            LastByteSyncEngine,
            build_raw_request,
        )

        findings: list[Finding] = []
        tested: list[dict] = []

        base_url = await resolve_base_url(target.host, ctx)
        if not base_url:
            return PluginResult.success(
                self.meta.name, target.host,
                findings=[Finding.info("Host not reachable")],
                data={"race_results": []},
            )

        # Resolve connection parameters
        parsed = urlparse(base_url)
        use_tls = parsed.scheme == "https"
        port = parsed.port or (443 if use_tls else 80)
        hostname = parsed.hostname or target.host

        # Collect target endpoints
        endpoints = self._select_endpoints(target.host, ctx)

        # Inject auth headers if available
        auth_headers: dict[str, str] = {}
        if ctx.auth:
            auth_headers = ctx.auth.inject(target.host) or {}

        engine = LastByteSyncEngine(
            num_connections=NUM_CONNECTIONS,
            connect_timeout=8.0,
            response_timeout=12.0,
        )

        for ep in endpoints:
            if ctx.should_stop or len(findings) >= 3:
                break

            path = ep["path"]
            method = ep.get("method", "POST").upper()
            body = ep.get("body", "")

            request_bytes = build_raw_request(
                method, path, hostname,
                headers=auth_headers if auth_headers else None,
                body=body,
            )

            try:
                result = await engine.execute(
                    hostname, port, request_bytes, use_tls=use_tls,
                )
            except Exception:
                continue

            # Analyze results
            analysis = self._analyze_burst(result, path)
            tested.append(analysis)

            if analysis.get("finding"):
                findings.append(analysis["finding"])

        if not findings:
            findings.append(Finding.info(
                "No race conditions detected",
                tags=["pentesting", "race-condition"],
            ))

        return PluginResult.success(
            self.meta.name, target.host,
            findings=findings,
            data={
                "race_results": [
                    {k: v for k, v in t.items() if k != "finding"}
                    for t in tested
                ],
                "endpoints_tested": len(tested),
                "burst_size": NUM_CONNECTIONS,
            },
        )

    def _select_endpoints(self, host: str, ctx) -> list[dict[str, str]]:
        """Select target endpoints from OpenAPI data or fallback paths."""
        endpoints: list[dict[str, str]] = []

        # Try OpenAPI endpoints first
        api_endpoints = ctx.state.get("api_endpoints_detailed", {}).get(host, [])
        for ep in api_endpoints:
            method = ep.get("method", "get").upper()
            if method not in ("POST", "PUT", "PATCH"):
                continue
            path = ep.get("path", "")
            summary = ep.get("summary", "") + " " + " ".join(ep.get("tags", []))
            if STATEFUL_KEYWORDS.search(path + " " + summary):
                body_hint = ""
                for param in ep.get("parameters", []):
                    if param.get("in") in ("query", "body"):
                        body_hint += f"{param['name']}=test&"
                endpoints.append({
                    "path": path,
                    "method": method,
                    "body": body_hint.rstrip("&"),
                    "source": "openapi",
                })
            if len(endpoints) >= 15:
                break

        # Fallback to hardcoded paths
        if not endpoints:
            endpoints = [dict(ep) for ep in RACE_PATHS]

        # Also add discovered API paths if available
        extra_paths = ctx.state.get("discovered_api_paths", {}).get(host, [])
        for p in extra_paths[:5]:
            path = p if isinstance(p, str) else p.get("path", "")
            if path and STATEFUL_KEYWORDS.search(path):
                endpoints.append({
                    "path": path, "method": "POST", "body": "",
                    "source": "discovered",
                })

        return endpoints[:20]

    def _analyze_burst(
        self, result, path: str,
    ) -> dict:
        """Analyze burst responses for race condition indicators."""
        analysis: dict = {
            "path": path,
            "total": result.total_connections,
            "ready": result.ready_connections,
            "failed": result.failed_connections,
            "jitter_ms": round(result.sync_jitter_ms, 2),
            "finding": None,
        }

        valid = [r for r in result.responses if r.status > 0]
        if not valid:
            return analysis

        # Count statuses
        status_counts = Counter(r.status for r in valid)
        analysis["status_distribution"] = dict(status_counts)

        # Count success/failure patterns
        success_count = 0
        failure_count = 0
        for r in valid:
            if r.status in (200, 201, 204):
                if any(p.search(r.body) for p in SUCCESS_PATTERNS):
                    success_count += 1
                elif not any(p.search(r.body) for p in FAILURE_PATTERNS):
                    success_count += 1  # 200 without failure = success
            if any(p.search(r.body) for p in FAILURE_PATTERNS):
                failure_count += 1

        analysis["success_count"] = success_count
        analysis["failure_count"] = failure_count

        # Detection: multiple successes = race condition
        if success_count > 1:
            evidence = (
                f"Path: {path}\n"
                f"Burst size: {result.total_connections}\n"
                f"Sync jitter: {result.sync_jitter_ms:.1f}ms\n"
                f"Success responses: {success_count}/{len(valid)}\n"
                f"Status distribution: {dict(status_counts)}\n"
                f"Sample success body: {valid[0].body[:200]}"
            )
            analysis["finding"] = Finding.critical(
                f"Race condition: {path} ({success_count} duplicate successes)",
                description=(
                    f"Single-packet attack with {result.total_connections} "
                    f"simultaneous requests resulted in {success_count} "
                    f"successful operations where only 1 should be allowed."
                ),
                evidence=evidence,
                remediation=(
                    "Implement proper mutex/locking on stateful operations. "
                    "Use database-level transactions with serializable isolation. "
                    "Apply idempotency keys for financial operations."
                ),
                confidence=0.9,
                verified=True,
                tags=["pentesting", "race-condition", "single-packet"],
            )
            return analysis

        # Detection: inconsistent statuses
        if len(status_counts) >= 3:
            evidence = (
                f"Path: {path}\n"
                f"Status distribution: {dict(status_counts)}\n"
                f"Sync jitter: {result.sync_jitter_ms:.1f}ms"
            )
            analysis["finding"] = Finding.high(
                f"Potential race condition: {path} (inconsistent responses)",
                description=(
                    f"Simultaneous requests produced {len(status_counts)} "
                    f"distinct status codes, suggesting non-atomic processing."
                ),
                evidence=evidence,
                remediation="Review endpoint concurrency handling.",
                confidence=0.6,
                false_positive_risk="medium",
                tags=["pentesting", "race-condition"],
            )
            return analysis

        # Detection: timing jitter with mixed results
        if (
            result.sync_jitter_ms < 50
            and success_count >= 1
            and failure_count >= 1
        ):
            evidence = (
                f"Path: {path}\n"
                f"Jitter: {result.sync_jitter_ms:.1f}ms\n"
                f"Success: {success_count}, Failure: {failure_count}\n"
                f"Status distribution: {dict(status_counts)}"
            )
            analysis["finding"] = Finding.medium(
                f"Possible race window: {path}",
                description=(
                    "Low-jitter burst produced mixed success/failure "
                    "responses, suggesting a narrow race window."
                ),
                evidence=evidence,
                remediation="Test with higher concurrency.",
                confidence=0.4,
                false_positive_risk="high",
                tags=["pentesting", "race-condition"],
            )

        return analysis
