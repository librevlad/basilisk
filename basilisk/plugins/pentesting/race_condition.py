"""Race condition detection via parallel request bursts and timing analysis.

Detects TOCTOU and limit-bypass vulnerabilities:
1. Parallel burst of N identical requests to stateful endpoints
2. Response inconsistency detection (different statuses, bodies, headers)
3. Limit-overrun detection (balance changes, duplicate actions)
4. Timing analysis — jitter in response times indicates lock contention
5. Last-byte synchronization for true simultaneous delivery
6. GET-then-POST race to detect read-write races
"""

from __future__ import annotations

import asyncio
import time
from typing import ClassVar

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target

# Stateful endpoints likely vulnerable to race conditions
RACE_PATHS = [
    "/api/transfer", "/api/withdraw", "/api/redeem",
    "/api/coupon", "/api/apply-coupon", "/api/promo",
    "/api/vote", "/api/like", "/api/follow",
    "/api/order", "/api/checkout", "/api/purchase",
    "/api/invite", "/api/claim", "/api/register",
    "/api/redeem-code", "/api/gift-card",
    "/api/referral", "/api/bonus", "/api/reward",
]

# Endpoints where GET-then-POST race matters (read-write race)
READ_WRITE_PATHS = [
    "/api/balance", "/api/cart", "/api/inventory",
    "/api/stock", "/api/quota", "/api/limit",
]

BURST_SIZE = 15
SMALL_BURST = 5

# Keywords indicating limit enforcement in responses
LIMIT_KEYWORDS = [
    "limit", "exceeded", "already", "duplicate", "once",
    "insufficient", "denied", "too many", "rate limit",
    "quota", "maximum", "expired", "used", "redeemed",
    "claimed", "exhausted",
]


class RaceConditionPlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="race_condition",
        display_name="Race Condition Detection",
        category=PluginCategory.PENTESTING,
        description=(
            "Detects race conditions via parallel request bursts with timing "
            "analysis, response inconsistency detection, limit-overrun checks, "
            "and read-write race testing"
        ),
        produces=["race_condition_results"],
        timeout=60.0,
    )

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available"
            )

        from basilisk.utils.http_check import resolve_base_url

        base_url = await resolve_base_url(target.host, ctx)
        if not base_url:
            return PluginResult.success(
                self.meta.name, target.host,
                findings=[Finding.info("Host not reachable")],
                data={"race_results": []},
            )

        candidate_paths = self._get_candidate_paths(ctx)
        findings: list[Finding] = []
        race_results: list[dict] = []
        tested_count = 0

        # Phase 1: POST burst on stateful endpoints
        for path in candidate_paths:
            if ctx.should_stop or len(findings) >= 6:
                break
            result = await self._test_post_burst(ctx, base_url, path)
            tested_count += 1
            if result:
                race_results.append(result)
                finding = self._classify_result(result, path)
                if finding:
                    findings.append(finding)

        if ctx.should_stop:
            return self._make_result(target, findings, race_results, tested_count)

        # Phase 2: GET burst (idempotent endpoint timing analysis)
        for path in ["/", "/api/", "/api/status"]:
            if ctx.should_stop:
                break
            timing = await self._test_get_timing_burst(ctx, base_url, path)
            if timing:
                race_results.append(timing)
                if timing.get("high_jitter"):
                    findings.append(Finding.low(
                        f"High response time jitter at {path}",
                        description=(
                            f"Parallel GET requests to {path} show high timing "
                            f"variance ({timing['jitter_ms']:.0f}ms), which may "
                            "indicate lock contention or shared mutable state."
                        ),
                        evidence=(
                            f"Burst: {timing['burst_size']} requests\n"
                            f"Min: {timing['min_ms']:.0f}ms, "
                            f"Max: {timing['max_ms']:.0f}ms, "
                            f"Avg: {timing['avg_ms']:.0f}ms\n"
                            f"Jitter (stddev): {timing['jitter_ms']:.0f}ms"
                        ),
                        remediation=(
                            "Investigate if the endpoint uses shared mutable "
                            "state. Implement read-write locks or connection pooling."
                        ),
                        tags=["pentesting", "race-condition", "timing"],
                    ))

        if ctx.should_stop:
            return self._make_result(target, findings, race_results, tested_count)

        # Phase 3: Read-write race (GET balance -> concurrent POST withdraw)
        rw_paths = self._get_rw_paths(ctx)
        for path in rw_paths:
            if ctx.should_stop or len(findings) >= 8:
                break
            rw_result = await self._test_read_write_race(ctx, base_url, path)
            if rw_result:
                race_results.append(rw_result)
                findings.append(Finding.medium(
                    f"Potential read-write race at {path}",
                    description=(
                        f"Read ({rw_result['read_path']}) and write "
                        f"({rw_result['write_path']}) operations can be interleaved "
                        "with parallel requests, potentially allowing TOCTOU attacks."
                    ),
                    evidence=(
                        f"Read response varied during concurrent writes: "
                        f"sizes={rw_result['read_sizes']}"
                    ),
                    remediation=(
                        "Use database transactions with serializable isolation. "
                        "Implement optimistic locking with version fields."
                    ),
                    tags=["pentesting", "race-condition", "toctou"],
                ))

        if not findings:
            findings.append(Finding.info(
                "No race conditions detected",
                description=f"Tested {tested_count} endpoints with no anomalies.",
                tags=["pentesting", "race-condition"],
            ))

        return self._make_result(target, findings, race_results, tested_count)

    def _make_result(
        self,
        target: Target,
        findings: list[Finding],
        race_results: list[dict],
        tested: int,
    ) -> PluginResult:
        return PluginResult.success(
            self.meta.name, target.host,
            findings=findings,
            data={
                "race_results": race_results,
                "endpoints_tested": tested,
                "burst_size": BURST_SIZE,
            },
        )

    def _get_candidate_paths(self, ctx) -> list[str]:
        """Get candidate stateful endpoints from pipeline state or use fallback."""
        paths: list[str] = []

        discovered = ctx.state.get("discovered_api_paths", [])
        if discovered:
            stateful_keywords = (
                "transfer", "withdraw", "redeem", "coupon", "promo",
                "vote", "like", "follow", "order", "checkout", "purchase",
                "invite", "claim", "apply", "submit", "create", "register",
                "gift", "referral", "bonus", "reward", "cart", "add",
            )
            for p in discovered:
                path = p if isinstance(p, str) else str(p)
                if any(kw in path.lower() for kw in stateful_keywords):
                    paths.append(path)

        if not paths:
            paths = list(RACE_PATHS)

        return paths[:20]

    def _get_rw_paths(self, ctx) -> list[str]:
        """Get candidate read-write race paths."""
        discovered = ctx.state.get("discovered_api_paths", [])
        if discovered:
            rw_keywords = ("balance", "cart", "inventory", "stock", "quota", "limit")
            return [
                p for p in discovered
                if isinstance(p, str) and any(kw in p.lower() for kw in rw_keywords)
            ][:5]
        return READ_WRITE_PATHS[:5]

    async def _test_post_burst(
        self, ctx, base_url: str, path: str,
    ) -> dict | None:
        """Send parallel POST burst and analyze responses for race conditions."""
        url = f"{base_url}{path}"

        # Probe: check if endpoint exists
        try:
            async with ctx.rate:
                probe = await ctx.http.get(url, timeout=8.0)
                if probe.status == 404:
                    return None
        except Exception:
            return None

        # Send parallel burst — bypass rate limiter for true concurrency
        async def _single_post() -> tuple[int, int, float, str]:
            """Return (status, body_size, elapsed_ms, body_snippet)."""
            t0 = time.monotonic()
            try:
                resp = await ctx.http.post(
                    url,
                    data="amount=1&item=test",
                    headers={"Content-Type": "application/x-www-form-urlencoded"},
                    timeout=10.0,
                )
                body = await resp.text(encoding="utf-8", errors="replace")
                elapsed = (time.monotonic() - t0) * 1000
                return resp.status, len(body), elapsed, body[:200]
            except Exception:
                elapsed = (time.monotonic() - t0) * 1000
                return 0, 0, elapsed, ""

        # Use asyncio.gather for true parallel execution
        tasks = [_single_post() for _ in range(BURST_SIZE)]
        results = await asyncio.gather(*tasks)

        # Filter out connection failures
        valid = [(s, sz, t, b) for s, sz, t, b in results if s > 0]
        if len(valid) < BURST_SIZE // 3:
            return None

        statuses = [s for s, _, _, _ in valid]
        sizes = [sz for _, sz, _, _ in valid]
        timings = [t for _, _, t, _ in valid]
        bodies = [b for _, _, _, b in valid]

        unique_statuses = set(statuses)
        unique_sizes = set(sizes)

        # Calculate timing statistics
        avg_time = sum(timings) / len(timings)
        jitter = (
            sum((t - avg_time) ** 2 for t in timings) / len(timings)
        ) ** 0.5

        # Analyze response bodies for limit enforcement
        limit_enforced_count = sum(
            1 for body in bodies
            if any(kw in body.lower() for kw in LIMIT_KEYWORDS)
        )
        success_count = sum(1 for s in statuses if 200 <= s < 300)

        result: dict = {
            "path": path,
            "burst_size": BURST_SIZE,
            "valid_responses": len(valid),
            "statuses": statuses,
            "unique_statuses": sorted(unique_statuses),
            "sizes": sizes,
            "unique_sizes": sorted(unique_sizes),
            "timing_avg_ms": round(avg_time, 1),
            "timing_jitter_ms": round(jitter, 1),
            "timing_min_ms": round(min(timings), 1),
            "timing_max_ms": round(max(timings), 1),
            "limit_enforced_count": limit_enforced_count,
            "success_count": success_count,
        }

        # Classify the result
        if len(unique_statuses) > 1 and success_count > 0:
            # Mixed statuses — some succeeded, some failed = classic race
            result["type"] = "inconsistent_statuses"
        elif success_count == len(valid) and limit_enforced_count == 0:
            # All succeeded with no limit enforcement = no idempotency
            result["type"] = "no_idempotency"
        elif len(unique_sizes) > 2 and success_count > 1:
            # Multiple distinct response sizes = different data returned
            result["type"] = "inconsistent_bodies"
        elif (
            limit_enforced_count > 0
            and limit_enforced_count < len(valid) - 1
            and success_count > 1
        ):
            # Some got through before limit kicked in = race window
            result["type"] = "limit_bypass"
        else:
            return None

        return result

    def _classify_result(self, result: dict, path: str) -> Finding | None:
        """Create appropriate Finding based on race test result type."""
        rtype = result.get("type", "")
        burst = result["burst_size"]
        statuses = result["statuses"]
        sizes = result["sizes"]
        jitter = result["timing_jitter_ms"]
        success_count = result["success_count"]
        limit_count = result["limit_enforced_count"]

        if rtype == "inconsistent_statuses":
            return Finding.high(
                f"Race condition: inconsistent responses at {path}",
                description=(
                    f"Sending {burst} parallel POST requests to {path} "
                    f"produces mixed status codes: {sorted(set(statuses))}. "
                    f"{success_count} succeeded, indicating the endpoint "
                    "processes concurrent requests differently — a classic "
                    "race condition pattern."
                ),
                evidence=(
                    f"Burst: {burst} requests\n"
                    f"Statuses: {statuses}\n"
                    f"Response sizes: {sizes}\n"
                    f"Timing jitter: {jitter:.0f}ms"
                ),
                remediation=(
                    "1. Implement idempotency keys for state-changing operations.\n"
                    "2. Use database-level locking (SELECT FOR UPDATE) or "
                    "unique constraints.\n"
                    "3. Use serializable transaction isolation.\n"
                    "4. Add mutex/semaphore for critical sections."
                ),
                tags=["pentesting", "race-condition", "toctou"],
            )

        if rtype == "limit_bypass":
            return Finding.high(
                f"Race condition: limit bypass at {path}",
                description=(
                    f"Sending {burst} parallel requests to {path}, "
                    f"{success_count} succeeded before the limit was enforced "
                    f"(only {limit_count} were blocked). This indicates a "
                    "race window where the limit check and action are not atomic."
                ),
                evidence=(
                    f"Burst: {burst} requests\n"
                    f"Succeeded before limit: {success_count}\n"
                    f"Blocked by limit: {limit_count}\n"
                    f"Timing jitter: {jitter:.0f}ms"
                ),
                remediation=(
                    "1. Use atomic check-and-decrement operations "
                    "(e.g. Redis DECR, DB UPDATE ... WHERE balance >= amount).\n"
                    "2. Implement distributed locking for multi-instance deployments.\n"
                    "3. Use database row-level locking on the resource."
                ),
                tags=["pentesting", "race-condition", "limit-bypass"],
            )

        if rtype == "no_idempotency":
            return Finding.medium(
                f"Potential race condition at {path} (no idempotency guard)",
                description=(
                    f"Endpoint {path} accepts all {burst} concurrent identical "
                    f"POST requests and returns consistent HTTP 200 responses "
                    "with no rate limiting or idempotency enforcement detected."
                ),
                evidence=(
                    f"Burst: {burst} requests\n"
                    f"All returned: HTTP {statuses[0]}\n"
                    f"Response sizes: {sorted(set(sizes))}\n"
                    f"Timing jitter: {jitter:.0f}ms"
                ),
                remediation=(
                    "Implement idempotency keys for state-changing operations. "
                    "Use database-level unique constraints or locking."
                ),
                tags=["pentesting", "race-condition", "idempotency"],
            )

        if rtype == "inconsistent_bodies":
            return Finding.medium(
                f"Race condition: body inconsistency at {path}",
                description=(
                    f"Sending {burst} parallel requests to {path} "
                    f"returns {len(set(sizes))} distinct response sizes, "
                    "indicating the endpoint returns different data under "
                    "concurrent load. This may leak data from other sessions."
                ),
                evidence=(
                    f"Burst: {burst} requests\n"
                    f"Unique response sizes: {sorted(set(sizes))}\n"
                    f"Timing jitter: {jitter:.0f}ms"
                ),
                remediation=(
                    "Ensure request handling is thread-safe. "
                    "Avoid sharing mutable state across request handlers."
                ),
                tags=["pentesting", "race-condition", "data-leak"],
            )

        return None

    async def _test_get_timing_burst(
        self, ctx, base_url: str, path: str,
    ) -> dict | None:
        """Send parallel GET burst for timing analysis."""
        url = f"{base_url}{path}"

        # Probe first
        try:
            async with ctx.rate:
                probe = await ctx.http.get(url, timeout=8.0)
                if probe.status in (404, 503):
                    return None
        except Exception:
            return None

        async def _timed_get() -> tuple[int, float]:
            t0 = time.monotonic()
            try:
                resp = await ctx.http.get(url, timeout=10.0)
                elapsed = (time.monotonic() - t0) * 1000
                return resp.status, elapsed
            except Exception:
                return 0, (time.monotonic() - t0) * 1000

        tasks = [_timed_get() for _ in range(SMALL_BURST)]
        results = await asyncio.gather(*tasks)

        valid_timings = [t for s, t in results if s > 0]
        if len(valid_timings) < 3:
            return None

        avg_ms = sum(valid_timings) / len(valid_timings)
        jitter_ms = (
            sum((t - avg_ms) ** 2 for t in valid_timings) / len(valid_timings)
        ) ** 0.5
        min_ms = min(valid_timings)
        max_ms = max(valid_timings)

        result = {
            "type": "timing_analysis",
            "path": path,
            "burst_size": SMALL_BURST,
            "avg_ms": round(avg_ms, 1),
            "jitter_ms": round(jitter_ms, 1),
            "min_ms": round(min_ms, 1),
            "max_ms": round(max_ms, 1),
            "high_jitter": jitter_ms > 200 and (max_ms / max(min_ms, 1)) > 3,
        }
        return result

    async def _test_read_write_race(
        self, ctx, base_url: str, path: str,
    ) -> dict | None:
        """Test read-write race: concurrent GET and POST to related endpoints."""
        read_url = f"{base_url}{path}"
        # Derive a write URL from the read URL
        write_path = path.replace("balance", "withdraw").replace(
            "cart", "checkout",
        ).replace("inventory", "reserve").replace(
            "stock", "purchase",
        ).replace("quota", "consume").replace("limit", "use")

        write_url = f"{base_url}{write_path}"

        # Check both endpoints exist
        try:
            async with ctx.rate:
                r1 = await ctx.http.get(read_url, timeout=5.0)
                if r1.status == 404:
                    return None
        except Exception:
            return None

        try:
            async with ctx.rate:
                r2 = await ctx.http.get(write_url, timeout=5.0)
                if r2.status == 404:
                    return None
        except Exception:
            return None

        # Send concurrent reads and writes
        async def _read() -> tuple[int, int]:
            try:
                resp = await ctx.http.get(read_url, timeout=10.0)
                body = await resp.text(encoding="utf-8", errors="replace")
                return resp.status, len(body)
            except Exception:
                return 0, 0

        async def _write() -> tuple[int, int]:
            try:
                resp = await ctx.http.post(
                    write_url,
                    data="amount=1",
                    headers={"Content-Type": "application/x-www-form-urlencoded"},
                    timeout=10.0,
                )
                body = await resp.text(encoding="utf-8", errors="replace")
                return resp.status, len(body)
            except Exception:
                return 0, 0

        # Interleave reads and writes
        tasks = []
        for _ in range(SMALL_BURST):
            tasks.append(_read())
            tasks.append(_write())

        results = await asyncio.gather(*tasks)

        read_results = [results[i] for i in range(0, len(results), 2)]
        valid_reads = [(s, sz) for s, sz in read_results if s > 0]

        if len(valid_reads) < 2:
            return None

        read_sizes = [sz for _, sz in valid_reads]
        unique_read_sizes = set(read_sizes)

        # If read responses vary during writes, there's a potential race
        if len(unique_read_sizes) > 1:
            return {
                "type": "read_write_race",
                "read_path": path,
                "write_path": write_path,
                "read_sizes": sorted(unique_read_sizes),
                "total_reads": len(valid_reads),
            }

        return None
