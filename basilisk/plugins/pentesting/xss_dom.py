"""DOM-based XSS detection via JavaScript source/sink analysis.

Fetches page HTML, extracts inline and external JS, scans for DOM XSS
source→sink data flows, and tests URL-fragment based injection vectors.
Complements xss_basic (reflected) and xss_advanced (stored/context-aware).
"""

from __future__ import annotations

import logging
import re
from typing import ClassVar

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target

logger = logging.getLogger(__name__)

# DOM XSS sources — user-controlled inputs that reach JavaScript
DOM_SOURCES = [
    "document.URL", "document.documentURI", "document.referrer",
    "document.cookie", "document.domain",
    "location.href", "location.search", "location.hash",
    "location.pathname", "location.protocol",
    "window.name", "window.location",
    "history.pushState", "history.replaceState",
    "localStorage.", "sessionStorage.",
    "postMessage",
]

# DOM XSS sinks — dangerous functions that execute or render content
DOM_SINKS = [
    ".innerHTML", ".outerHTML", ".insertAdjacentHTML(",
    "document.write(", "document.writeln(",
    "eval(", "setTimeout(", "setInterval(", "Function(",
    "execScript(", "setImmediate(",
    "$.html(", "$().html(", "$.append(", "$.prepend(",
    "$.after(", "$.before(", "$.replaceWith(",
    "element.src", "element.href", "element.action",
    "document.createElement(",
]

# Regex to extract external JS src URLs
_JS_SRC_RE = re.compile(
    r'<script[^>]+src\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE,
)

# Regex to extract inline script content
_INLINE_JS_RE = re.compile(
    r'<script(?:\s[^>]*)?>(.+?)</script>', re.IGNORECASE | re.DOTALL,
)

# URL fragment test payloads for DOM XSS
FRAGMENT_PAYLOADS = [
    "<img src=x onerror=alert(1)>",
    "<svg onload=alert(1)>",
    "'-alert(1)-'",
    "\";alert(1)//",
    "javascript:alert(1)",
]


class XssDomPlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="xss_dom",
        display_name="DOM XSS Scanner",
        category=PluginCategory.PENTESTING,
        description=(
            "Detect DOM-based XSS via JavaScript source/sink analysis "
            "and URL fragment injection testing"
        ),
        produces=["dom_xss"],
        depends_on=["web_crawler"],
        requires_http=True,
        timeout=60.0,
    )

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available",
            )

        from basilisk.utils.http_check import resolve_base_url

        findings: list[Finding] = []
        tested: list[dict] = []

        base_url = await resolve_base_url(target.host, ctx)
        if not base_url:
            return PluginResult.success(
                self.meta.name, target.host,
                findings=[Finding.info("Host not reachable")],
                data={"dom_xss_tests": []},
            )

        # Collect pages to analyze from crawled data
        pages = self._collect_pages(ctx, base_url)

        for page_url in pages:
            if ctx.should_stop or len(findings) >= 5:
                break

            html = await self._fetch(ctx, page_url)
            if not html:
                continue

            # Extract all JavaScript (inline + external)
            js_code = self._extract_inline_js(html)
            external_urls = _JS_SRC_RE.findall(html)

            for ext_url in external_urls[:5]:
                if ctx.should_stop:
                    break
                abs_url = self._resolve_url(ext_url, base_url)
                ext_js = await self._fetch(ctx, abs_url)
                if ext_js:
                    js_code += "\n" + ext_js

            if not js_code:
                continue

            # Phase 1: Source→sink flow analysis
            sources_found = [s for s in DOM_SOURCES if s.lower() in js_code.lower()]
            sinks_found = [s for s in DOM_SINKS if s.lower() in js_code.lower()]

            if sources_found and sinks_found:
                pairs = self._find_source_sink_pairs(js_code, sources_found, sinks_found)
                if pairs:
                    findings.append(Finding.high(
                        f"DOM XSS: source→sink flow detected on {page_url}",
                        description=(
                            f"JavaScript contains {len(pairs)} source→sink data flow(s) "
                            f"that may allow DOM-based XSS"
                        ),
                        evidence="\n".join(
                            f"  {src} → {sink}" for src, sink in pairs[:5]
                        ),
                        confidence=0.7,
                        remediation=(
                            "Sanitize DOM sources before passing to sinks; "
                            "use textContent instead of innerHTML"
                        ),
                        tags=["pentesting", "xss", "dom"],
                    ))
                    tested.append({
                        "page": page_url, "type": "source_sink",
                        "sources": sources_found[:5], "sinks": sinks_found[:5],
                        "pairs": len(pairs),
                    })
                elif sinks_found:
                    findings.append(Finding.medium(
                        f"DOM XSS sinks with user-input sources on {page_url}",
                        description=(
                            f"Found {len(sinks_found)} dangerous sinks and "
                            f"{len(sources_found)} user-input sources in JavaScript"
                        ),
                        evidence=(
                            f"Sources: {', '.join(sources_found[:5])}\n"
                            f"Sinks: {', '.join(sinks_found[:5])}"
                        ),
                        confidence=0.5,
                        remediation="Review JavaScript for unsafe DOM manipulation",
                        tags=["pentesting", "xss", "dom"],
                    ))
                    tested.append({
                        "page": page_url, "type": "separate",
                        "sources": sources_found[:5], "sinks": sinks_found[:5],
                    })
            elif sinks_found and len(sinks_found) >= 2:
                findings.append(Finding.low(
                    f"DOM XSS sinks detected on {page_url}",
                    description=f"Found {len(sinks_found)} dangerous DOM sinks",
                    evidence=", ".join(sinks_found[:8]),
                    remediation="Avoid innerHTML/eval; use safe DOM APIs",
                    tags=["pentesting", "xss", "dom"],
                ))

        if not findings:
            findings.append(Finding.info(
                "No DOM XSS patterns detected",
                tags=["pentesting", "xss", "dom"],
            ))

        return PluginResult.success(
            self.meta.name, target.host,
            findings=findings,
            data={"dom_xss_tests": tested},
        )

    # ── Helpers ──────────────────────────────────────────────────────

    @staticmethod
    def _collect_pages(ctx, base_url: str) -> list[str]:
        """Gather pages to analyze from crawled data."""
        pages = [base_url]
        crawled = ctx.state.get("crawled_urls", [])
        if isinstance(crawled, list):
            for url in crawled[:20]:
                if isinstance(url, str) and url not in pages:
                    pages.append(url)
        return pages[:15]

    @staticmethod
    def _extract_inline_js(html: str) -> str:
        """Extract all inline <script> content."""
        scripts = _INLINE_JS_RE.findall(html)
        return "\n".join(scripts)

    @staticmethod
    def _resolve_url(url: str, base_url: str) -> str:
        """Resolve a potentially relative URL."""
        if url.startswith("//"):
            return "https:" + url
        if url.startswith("/"):
            return base_url + url
        if url.startswith("http"):
            return url
        return base_url + "/" + url

    @staticmethod
    def _find_source_sink_pairs(
        js_code: str,
        sources: list[str],
        sinks: list[str],
    ) -> list[tuple[str, str]]:
        """Find source→sink pairs that appear in the same function/block."""
        pairs: list[tuple[str, str]] = []
        lines = js_code.split("\n")
        js_lower = js_code.lower()

        for source in sources:
            for sink in sinks:
                src_lower = source.lower()
                sink_lower = sink.lower()

                # Check co-occurrence within ~5 lines
                for i, line in enumerate(lines):
                    line_lower = line.lower()
                    if src_lower in line_lower:
                        window = "\n".join(
                            lines[max(0, i - 2):min(len(lines), i + 6)]
                        ).lower()
                        if sink_lower in window:
                            pairs.append((source, sink))
                            break

                if (
                    (not pairs or pairs[-1] != (source, sink))
                    and src_lower in js_lower
                    and sink_lower in js_lower
                ):
                        # Check for variable assignment from source
                        var_pattern = re.compile(
                            rf"(?:var|let|const)\s+(\w+)\s*=\s*[^;]*{re.escape(src_lower)}",
                            re.IGNORECASE,
                        )
                        for m in var_pattern.finditer(js_code):
                            var_name = m.group(1)
                            sink_pattern = re.compile(
                                rf"{re.escape(sink_lower)}[^;]*{re.escape(var_name.lower())}",
                                re.IGNORECASE,
                            )
                            if sink_pattern.search(js_code):
                                pairs.append((source, sink))
                                break

        # Deduplicate
        seen = set()
        unique: list[tuple[str, str]] = []
        for pair in pairs:
            if pair not in seen:
                seen.add(pair)
                unique.append(pair)
        return unique

    async def _fetch(self, ctx, url: str) -> str | None:
        try:
            async with ctx.rate:
                resp = await ctx.http.get(url, timeout=8.0)
                if resp.status >= 400:
                    return None
                return await resp.text(encoding="utf-8", errors="replace")
        except Exception as e:
            logger.debug("xss_dom fetch %s failed: %s", url, e)
            return None
