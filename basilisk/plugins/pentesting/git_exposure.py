"""Git repository exposure detection — git-dumper level depth.

Goes beyond simple /.git/HEAD checks:
1. Validates every git endpoint with content-aware body checks
2. Parses .git/config for remote URLs, usernames, emails
3. Enumerates pack index files for object-level access
4. Reads .git/logs/HEAD for commit history extraction
5. Probes refs/heads/ and refs/tags/ for branch/tag enumeration
6. Parses .gitignore for interesting excluded paths
7. Checks .git/info/exclude, .git/description, COMMIT_EDITMSG
8. Attempts HEAD -> commit -> tree chain reconstruction
9. SPA/soft-404 baseline filtering
"""

from __future__ import annotations

import re
from typing import ClassVar

from basilisk.core.plugin import BasePlugin, PluginCategory, PluginMeta
from basilisk.models.result import Finding, PluginResult
from basilisk.models.target import Target
from basilisk.utils.batch_check import batch_head_check
from basilisk.utils.http import resolve_base_urls

# Secret patterns for scanning downloaded content
_SECRET_PATTERNS: list[tuple[str, re.Pattern]] = [
    ("AWS Access Key", re.compile(r"AKIA[0-9A-Z]{16}")),
    ("GitHub Token", re.compile(r"ghp_[a-zA-Z0-9]{36}")),
    ("GitLab Token", re.compile(r"glpat-[a-zA-Z0-9\-]{20,}")),
    ("Slack Token", re.compile(r"xox[bpors]-[a-zA-Z0-9\-]+")),
    ("Google API Key", re.compile(r"AIza[0-9A-Za-z\-_]{35}")),
    ("OpenAI API Key", re.compile(r"sk-[a-zA-Z0-9]{32,}")),
    ("Stripe Secret Key", re.compile(r"sk_live_[0-9a-zA-Z]{24,}")),
    ("Stripe Publishable Key", re.compile(r"pk_live_[0-9a-zA-Z]{24,}")),
    ("Private Key", re.compile(r"-----BEGIN (?:RSA |EC |DSA )?PRIVATE KEY-----")),
    ("Heroku API Key", re.compile(r"(?:heroku.*['\"]?)[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}")),  # noqa: E501
    ("Twilio SID", re.compile(r"AC[a-zA-Z0-9]{32}")),
    ("SendGrid Key", re.compile(r"SG\.[a-zA-Z0-9\-_]{22,}\.[a-zA-Z0-9\-_]{43,}")),
    ("Generic High-Entropy Secret", re.compile(r"(?:secret|password|token|key|api_key)\s*[:=]\s*['\"][a-zA-Z0-9+/=!@#$%^&*]{20,}['\"]", re.IGNORECASE)),  # noqa: E501
    ("JWT Token", re.compile(r"eyJ[a-zA-Z0-9_-]{10,}\.eyJ[a-zA-Z0-9_-]{10,}\.[a-zA-Z0-9_-]{10,}")),  # noqa: E501
    ("Database URL", re.compile(r"(?:mysql|postgres|mongodb)://[^\s'\"]{10,}")),
]

# ---------------------------------------------------------------------------
# Path tiers — ordered by information value
# ---------------------------------------------------------------------------

# Tier 1: high-value git internals (credential/code leak risk)
GIT_TIER1 = [
    ".git/HEAD",
    ".git/config",
    ".git/index",
]

# Tier 2: commit history and ref enumeration
GIT_TIER2 = [
    ".git/logs/HEAD",
    ".git/logs/refs/heads/main",
    ".git/logs/refs/heads/master",
    ".git/refs/heads/main",
    ".git/refs/heads/master",
    ".git/refs/heads/develop",
    ".git/refs/heads/dev",
    ".git/refs/heads/staging",
    ".git/refs/heads/release",
    ".git/refs/tags/v1.0",
    ".git/refs/tags/v1.0.0",
    ".git/refs/tags/latest",
    ".git/packed-refs",
    ".git/COMMIT_EDITMSG",
    ".git/FETCH_HEAD",
    ".git/ORIG_HEAD",
]

# Tier 3: metadata and object access
GIT_TIER3 = [
    ".git/description",
    ".git/info/exclude",
    ".git/info/refs",
    ".git/objects/info/packs",
    ".git/objects/info/alternates",
    ".git/shallow",
    ".gitignore",
    ".gitattributes",
]

# Tier 4: other sensitive files (non-git VCS + environment)
OTHER_SENSITIVE = [
    (".env", "Environment file exposed"),
    (".env.local", "Local environment file exposed"),
    (".env.production", "Production environment file exposed"),
    (".env.staging", "Staging environment file exposed"),
    (".env.development", "Development environment file exposed"),
    (".env.backup", "Env backup exposed"),
    (".svn/entries", "SVN repository exposed"),
    (".svn/wc.db", "SVN database exposed"),
    (".hg/dirstate", "Mercurial repository exposed"),
    ("CVS/Root", "CVS root exposed"),
    (".DS_Store", "macOS .DS_Store exposed"),
    ("wp-config.php.bak", "WordPress config backup exposed"),
    ("web.config", "IIS web.config exposed"),
    (".htaccess", "Apache .htaccess exposed"),
    ("phpinfo.php", "phpinfo() exposed"),
    ("server-status", "Apache server-status exposed"),
    ("elmah.axd", "ELMAH error log exposed"),
]

# SHA-1 hex pattern
SHA1_RE = re.compile(r"[0-9a-f]{40}")
# .git/config remote URL patterns
REMOTE_URL_RE = re.compile(r"url\s*=\s*(.+)", re.MULTILINE)
EMAIL_RE = re.compile(r"email\s*=\s*(.+)", re.MULTILINE)
USERNAME_RE = re.compile(r"name\s*=\s*(.+)", re.MULTILINE)


class GitExposurePlugin(BasePlugin):
    meta: ClassVar[PluginMeta] = PluginMeta(
        name="git_exposure",
        display_name="Git/Sensitive File Exposure",
        category=PluginCategory.PENTESTING,
        description=(
            "Git-dumper level detection: config parsing, ref enumeration, "
            "object chain reconstruction, pack index check, credential extraction"
        ),
        produces=["exposed_files"],
        timeout=60.0,
    )

    async def run(self, target: Target, ctx) -> PluginResult:
        if ctx.http is None:
            return PluginResult.fail(
                self.meta.name, target.host, error="HTTP client not available"
            )

        base_urls = await resolve_base_urls(target, ctx)
        if not base_urls:
            return PluginResult.fail(
                self.meta.name, target.host, error="Host not reachable via HTTP(S)"
            )

        findings: list[Finding] = []
        exposed: list[dict] = []

        for base_url in base_urls:
            if ctx.should_stop:
                break
            await self._scan_base_url(base_url, target, ctx, findings, exposed)

        if not findings:
            findings.append(Finding.info(
                "No sensitive files exposed",
                tags=["pentesting", "exposure"],
            ))

        return PluginResult.success(
            self.meta.name, target.host,
            findings=findings,
            data={"exposed_files": exposed},
        )

    async def _scan_base_url(
        self, base_url: str, target, ctx,
        findings: list[Finding], exposed: list[dict],
    ) -> None:
        # SPA/soft-404 baseline detection
        spa_baseline = await self._get_spa_baseline(ctx, base_url)

        # Phase 1: Batch HEAD check on all git paths
        all_git_paths = GIT_TIER1 + GIT_TIER2 + GIT_TIER3
        git_urls = [f"{base_url}/{p}" for p in all_git_paths]
        other_urls = [f"{base_url}/{p}" for p, _ in OTHER_SENSITIVE]

        all_urls = git_urls + other_urls
        hits = await batch_head_check(
            ctx.http, all_urls, ctx.rate,
            concurrency=10, timeout=5.0,
            deadline=ctx._deadline,
        )

        # Phase 2: Deep validation with GET for each hit
        git_exposed = False

        for url, _status, _size in hits:
            if ctx.should_stop:
                break

            path = url[len(base_url) + 1:]
            body = await self._validated_get(ctx, url, path, spa_baseline)
            if body is None:
                continue

            exposed.append({"path": path, "url": url})

            if path.startswith(".git/"):
                git_exposed = True

            # Path-specific deep analysis
            if path == ".git/config":
                await self._analyze_git_config(
                    ctx, base_url, body, findings, exposed,
                )
            elif path == ".git/HEAD":
                await self._analyze_git_head(
                    ctx, base_url, body, findings, exposed, spa_baseline,
                )
            elif path == ".git/logs/HEAD":
                self._analyze_git_logs(body, findings)
            elif path == ".git/packed-refs":
                self._analyze_packed_refs(body, findings, exposed)
            elif path == ".git/objects/info/packs":
                await self._analyze_pack_index(
                    ctx, base_url, body, findings, exposed,
                )
            elif path == ".gitignore":
                self._analyze_gitignore(body, findings)
            elif path == ".git/info/exclude":
                self._analyze_gitignore(body, findings, label="git info/exclude")
            elif path == ".git/COMMIT_EDITMSG":
                self._report_commit_msg(body, findings)
            elif path == ".git/description":
                self._report_description(body, findings)
            else:
                # Generic sensitive file finding
                severity, desc = self._classify_path(path)
                finding_fn = {
                    "critical": Finding.critical,
                    "high": Finding.high,
                    "medium": Finding.medium,
                    "low": Finding.low,
                }.get(severity, Finding.info)
                findings.append(finding_fn(
                    desc,
                    description=f"Sensitive file accessible at {url}",
                    evidence=body[:300],
                    remediation=f"Block access to /{path} in web server config",
                    tags=["pentesting", "exposure"],
                ))

        # Summary finding for git exposure
        if git_exposed:
            git_paths = [
                e["path"] for e in exposed if e["path"].startswith(".git/")
            ]
            findings.insert(0, Finding.high(
                f"Git repository exposed ({len(git_paths)} files accessible)",
                description=(
                    f"The .git directory is publicly accessible. "
                    f"An attacker can reconstruct source code using tools "
                    f"like git-dumper. Exposed paths: {', '.join(git_paths[:10])}"
                ),
                evidence=f"Accessible git files: {', '.join(git_paths[:10])}",
                remediation=(
                    "Block access to the .git directory in web server config. "
                    "Nginx: location ~ /\\.git { deny all; }  "
                    "Apache: RedirectMatch 404 /\\.git"
                ),
                tags=["pentesting", "exposure", "git"],
            ))

    # ------------------------------------------------------------------
    # Deep analysis methods
    # ------------------------------------------------------------------

    def _scan_for_secrets(
        self, content: str, source: str, findings: list[Finding],
    ) -> int:
        """Scan text content for secrets. Returns count of secrets found."""
        found = 0
        for name, pattern in _SECRET_PATTERNS:
            matches = pattern.findall(content)
            for match in matches[:2]:
                found += 1
                redacted = match[:8] + "..." + match[-4:] if len(match) > 16 else "***"
                findings.append(Finding.critical(
                    f"Secret found in {source}: {name}",
                    description=(
                        f"A {name} was found in {source}. "
                        "Exposed credentials must be rotated immediately."
                    ),
                    evidence=(
                        f"Type: {name}\n"
                        f"Source: {source}\n"
                        f"Value (redacted): {redacted}"
                    ),
                    remediation=(
                        "Rotate the exposed credential immediately. "
                        "Remove sensitive data from git history using "
                        "git filter-branch or BFG Repo-Cleaner."
                    ),
                    confidence=0.9,
                    verified=True,
                    tags=["pentesting", "exposure", "secret-leak", "credential"],
                ))
        return found

    async def _analyze_git_config(
        self, ctx, base_url: str, body: str,
        findings: list[Finding], exposed: list[dict],
    ) -> None:
        """Parse .git/config for remote URLs, emails, usernames."""
        remotes = REMOTE_URL_RE.findall(body)
        emails = EMAIL_RE.findall(body)
        usernames = USERNAME_RE.findall(body)

        evidence_parts = [f"Config content (first 500 chars):\n{body[:500]}"]

        credential_leak = False
        if remotes:
            evidence_parts.append(f"Remote URLs: {', '.join(r.strip() for r in remotes)}")
            for remote in remotes:
                remote = remote.strip()
                # Check for embedded credentials in URLs
                if "@" in remote and "://" in remote:
                    before_at = remote.split("@")[0]
                    if ":" in before_at.split("//")[-1]:
                        credential_leak = True
                        evidence_parts.append(
                            f"CREDENTIAL IN URL: {remote[:80]}..."
                        )

        if emails:
            evidence_parts.append(
                f"Developer emails: {', '.join(e.strip() for e in emails)}"
            )
        if usernames:
            evidence_parts.append(
                f"Developer names: {', '.join(u.strip() for u in usernames)}"
            )

        if credential_leak:
            findings.append(Finding.critical(
                "Git config exposes credentials in remote URL",
                description=(
                    "The .git/config file contains remote URLs with "
                    "embedded credentials (username:password). "
                    "This is a direct credential leak."
                ),
                evidence="\n".join(evidence_parts),
                remediation=(
                    "Remove credentials from git remote URLs. "
                    "Use SSH keys or credential helpers instead. "
                    "Block .git directory access."
                ),
                tags=["pentesting", "exposure", "git", "credential-leak"],
            ))
        elif remotes or emails:
            findings.append(Finding.high(
                "Git config exposes repository metadata",
                description=(
                    "The .git/config file reveals remote repository URLs "
                    "and/or developer email addresses."
                ),
                evidence="\n".join(evidence_parts),
                remediation=(
                    "Block access to .git/config. "
                    "Review git remote URLs for sensitive information."
                ),
                tags=["pentesting", "exposure", "git"],
            ))

        # Scan for secrets in config content
        self._scan_for_secrets(body, ".git/config", findings)

    async def _analyze_git_head(
        self, ctx, base_url: str, body: str,
        findings: list[Finding], exposed: list[dict],
        spa_baseline: str,
    ) -> None:
        """Parse HEAD reference and attempt to follow commit chain."""
        body_stripped = body.strip()

        # HEAD format: "ref: refs/heads/main" or a direct SHA
        ref_match = re.match(r"ref:\s*(.+)", body_stripped)
        if ref_match:
            ref_path = ref_match.group(1).strip()
            findings.append(Finding.medium(
                f"Git HEAD points to {ref_path}",
                description=f"Current branch reference: {ref_path}",
                evidence=f"HEAD content: {body_stripped}",
                tags=["pentesting", "exposure", "git"],
            ))

            # Try to resolve the ref to a commit SHA
            ref_url = f"{base_url}/.git/{ref_path}"
            ref_body = await self._validated_get(
                ctx, ref_url, f".git/{ref_path}", spa_baseline,
            )
            if ref_body:
                sha = ref_body.strip()[:40]
                if SHA1_RE.match(sha):
                    exposed.append({
                        "path": f".git/{ref_path}", "url": ref_url,
                    })
                    # Try to read the commit object
                    await self._try_read_object(
                        ctx, base_url, sha, findings, exposed, spa_baseline,
                    )

        elif SHA1_RE.match(body_stripped[:40]):
            findings.append(Finding.medium(
                "Git HEAD is detached at a specific commit",
                description=f"Detached HEAD at commit {body_stripped[:40]}",
                evidence=f"HEAD content: {body_stripped[:40]}",
                tags=["pentesting", "exposure", "git"],
            ))
            await self._try_read_object(
                ctx, base_url, body_stripped[:40],
                findings, exposed, spa_baseline,
            )

    async def _try_read_object(
        self, ctx, base_url: str, sha: str,
        findings: list[Finding], exposed: list[dict],
        spa_baseline: str,
    ) -> None:
        """Attempt to read a loose git object by SHA."""
        if len(sha) < 40:
            return

        prefix = sha[:2]
        suffix = sha[2:]
        obj_url = f"{base_url}/.git/objects/{prefix}/{suffix}"

        try:
            async with ctx.rate:
                resp = await ctx.http.get(obj_url, timeout=5.0)
                if resp.status == 200:
                    # Object file exists — binary content
                    raw = await resp.read()
                    if raw and len(raw) > 2 and not self._is_spa_response(
                        raw.decode("utf-8", errors="replace"), spa_baseline
                    ):
                        exposed.append({
                            "path": f".git/objects/{prefix}/{suffix}",
                            "url": obj_url,
                        })
                        findings.append(Finding.high(
                            f"Git object accessible: {sha[:12]}...",
                            description=(
                                f"Loose git object {sha} is publicly "
                                f"readable. Full source reconstruction "
                                f"may be possible with git-dumper."
                            ),
                            evidence=f"Object URL: {obj_url}\nSize: {len(raw)} bytes",
                            remediation="Block access to .git/objects/ directory",
                            tags=["pentesting", "exposure", "git", "object"],
                        ))
        except Exception:
            pass

    def _analyze_git_logs(
        self, body: str, findings: list[Finding],
    ) -> None:
        """Extract commit history from .git/logs/HEAD."""
        commits = SHA1_RE.findall(body)
        unique_shas = list(dict.fromkeys(commits))[:20]

        if not unique_shas:
            return

        # Extract commit messages (format: SHA SHA author <email> timestamp +offset\tcommit: msg)
        log_lines = body.strip().splitlines()
        messages: list[str] = []
        emails: list[str] = []

        for line in log_lines[:20]:
            # Extract email from angle brackets
            email_match = re.search(r"<([^>]+)>", line)
            if email_match:
                emails.append(email_match.group(1))

            # Extract message after tab
            tab_idx = line.find("\t")
            if tab_idx >= 0:
                messages.append(line[tab_idx + 1:].strip())

        unique_emails = list(dict.fromkeys(emails))

        evidence = (
            f"Commits found: {len(unique_shas)}\n"
            f"SHAs: {', '.join(s[:8] for s in unique_shas[:10])}\n"
        )
        if unique_emails:
            evidence += f"Developer emails: {', '.join(unique_emails[:5])}\n"
        if messages:
            evidence += "Recent messages:\n" + "\n".join(
                f"  - {m}" for m in messages[:5]
            )

        findings.append(Finding.high(
            f"Git log exposed: {len(unique_shas)} commits, "
            f"{len(unique_emails)} developer emails",
            description=(
                "The git reflog is accessible, revealing commit history, "
                "developer emails, and commit messages."
            ),
            evidence=evidence,
            remediation="Block access to .git/logs/ directory",
            tags=["pentesting", "exposure", "git", "history"],
        ))

        # Scan for secrets in log content
        self._scan_for_secrets(body, ".git/logs/HEAD", findings)

    def _analyze_packed_refs(
        self, body: str,
        findings: list[Finding], exposed: list[dict],
    ) -> None:
        """Parse packed-refs for branch and tag enumeration."""
        branches: list[str] = []
        tags: list[str] = []

        for line in body.strip().splitlines():
            line = line.strip()
            if line.startswith("#") or not line:
                continue
            parts = line.split()
            if len(parts) >= 2:
                ref = parts[1]
                if ref.startswith("refs/heads/"):
                    branches.append(ref.replace("refs/heads/", ""))
                elif ref.startswith("refs/tags/"):
                    tags.append(ref.replace("refs/tags/", ""))

        if branches or tags:
            findings.append(Finding.medium(
                f"Git packed-refs: {len(branches)} branches, {len(tags)} tags",
                description=(
                    f"Packed refs reveal repository structure. "
                    f"Branches: {', '.join(branches[:10])}. "
                    f"Tags: {', '.join(tags[:10])}."
                ),
                evidence=(
                    f"Branches: {', '.join(branches[:10])}\n"
                    f"Tags: {', '.join(tags[:10])}"
                ),
                remediation="Block access to .git/packed-refs",
                tags=["pentesting", "exposure", "git"],
            ))

    async def _analyze_pack_index(
        self, ctx, base_url: str, body: str,
        findings: list[Finding], exposed: list[dict],
    ) -> None:
        """Parse objects/info/packs and check for accessible .idx/.pack files."""
        pack_names: list[str] = []
        for line in body.strip().splitlines():
            # Format: "P pack-<hex>.pack"
            if line.startswith("P "):
                pack_name = line[2:].strip()
                pack_names.append(pack_name)

        accessible_packs: list[str] = []
        for pack_name in pack_names[:5]:
            idx_name = pack_name.replace(".pack", ".idx")
            idx_url = f"{base_url}/.git/objects/pack/{idx_name}"
            try:
                async with ctx.rate:
                    resp = await ctx.http.head(idx_url, timeout=5.0)
                    if resp.status == 200:
                        accessible_packs.append(idx_name)
                        exposed.append({
                            "path": f".git/objects/pack/{idx_name}",
                            "url": idx_url,
                        })
            except Exception:
                continue

        if accessible_packs:
            findings.append(Finding.critical(
                f"Git pack index files accessible ({len(accessible_packs)} packs)",
                description=(
                    "Pack index files allow downloading the full repository "
                    "contents including all source code and history. "
                    "An attacker can use git-dumper to reconstruct the "
                    "entire repository."
                ),
                evidence=f"Accessible pack indices: {', '.join(accessible_packs)}",
                remediation="Block access to .git/objects/ directory entirely",
                tags=["pentesting", "exposure", "git", "pack"],
            ))

        # Try to download pack files and verify magic bytes
        for pack_name in pack_names[:3]:
            if ctx.should_stop:
                break
            pack_url = f"{base_url}/.git/objects/pack/{pack_name}"
            try:
                async with ctx.rate:
                    resp = await ctx.http.get(pack_url, timeout=10.0)
                    if resp.status == 200:
                        raw = await resp.read()
                        if raw[:4] == b"PACK":
                            exposed.append({
                                "path": f".git/objects/pack/{pack_name}",
                                "url": pack_url,
                                "size": len(raw),
                            })
                            findings.append(Finding.critical(
                                f"Git pack file downloadable: {pack_name}",
                                description=(
                                    f"Pack file ({len(raw)} bytes) with valid PACK "
                                    f"header is downloadable. Full repository "
                                    f"reconstruction is possible."
                                ),
                                evidence=(
                                    f"URL: {pack_url}\n"
                                    f"Size: {len(raw)} bytes\n"
                                    f"Magic: PACK (valid)"
                                ),
                                remediation="Block access to .git/objects/pack/",
                                confidence=0.95,
                                verified=True,
                                tags=["pentesting", "exposure", "git", "pack"],
                            ))
            except Exception:
                continue

    def _analyze_gitignore(
        self, body: str, findings: list[Finding],
        label: str = ".gitignore",
    ) -> None:
        """Analyze .gitignore for interesting excluded paths."""
        interesting_patterns = []
        sensitive_keywords = [
            "secret", "password", "credential", "key", "token",
            ".env", "config", "private", "backup", "dump",
            ".pem", ".key", ".p12", ".pfx", "id_rsa",
        ]

        for line in body.strip().splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            line_lower = line.lower()
            if any(kw in line_lower for kw in sensitive_keywords):
                interesting_patterns.append(line)

        if interesting_patterns:
            findings.append(Finding.medium(
                f"{label} reveals sensitive excluded paths",
                description=(
                    f"The {label} file hints at sensitive files that exist "
                    f"but are excluded from version control."
                ),
                evidence=(
                    "Interesting patterns:\n"
                    + "\n".join(f"  {p}" for p in interesting_patterns[:10])
                ),
                remediation=f"Block access to {label}",
                tags=["pentesting", "exposure", "git"],
            ))

    @staticmethod
    def _report_commit_msg(body: str, findings: list[Finding]) -> None:
        """Report exposed COMMIT_EDITMSG."""
        msg = body.strip()[:500]
        findings.append(Finding.low(
            "Git last commit message exposed",
            description="COMMIT_EDITMSG reveals the last commit message.",
            evidence=f"Message: {msg}",
            remediation="Block access to .git/COMMIT_EDITMSG",
            tags=["pentesting", "exposure", "git"],
        ))

    @staticmethod
    def _report_description(body: str, findings: list[Finding]) -> None:
        """Report exposed .git/description."""
        desc = body.strip()
        default = "Unnamed repository; edit this file 'description' to name the repository."
        if desc and desc != default:
            findings.append(Finding.low(
                "Git repository description exposed",
                description=f"Custom git description: {desc[:200]}",
                evidence=desc[:200],
                remediation="Block access to .git/description",
                tags=["pentesting", "exposure", "git"],
            ))

    # ------------------------------------------------------------------
    # Validation and classification helpers
    # ------------------------------------------------------------------

    async def _validated_get(
        self, ctx, url: str, path: str, spa_baseline: str,
    ) -> str | None:
        """GET a URL and validate that the response is real content."""
        try:
            async with ctx.rate:
                resp = await ctx.http.get(url, timeout=5.0)
                if resp.status != 200:
                    return None
                body = await resp.text(encoding="utf-8", errors="replace")

                if self._is_spa_response(body, spa_baseline):
                    return None
                if not self._is_real_content(path, body):
                    return None
                return body
        except Exception:
            return None

    @staticmethod
    async def _get_spa_baseline(ctx, base_url: str) -> str:
        """Fetch a nonexistent path to detect SPA catch-all responses."""
        try:
            async with ctx.rate:
                r = await ctx.http.get(
                    f"{base_url}/_nonexistent_8x7z_gitexp/", timeout=5.0,
                )
                if r.status == 200:
                    return await r.text(encoding="utf-8", errors="replace")
        except Exception:
            pass
        return ""

    @staticmethod
    def _is_spa_response(body: str, spa_baseline: str) -> bool:
        """Check if body matches the SPA baseline."""
        if not spa_baseline:
            return False
        return (
            abs(len(body) - len(spa_baseline)) < 200
            and body[:500] == spa_baseline[:500]
        )

    @staticmethod
    def _is_real_content(path: str, body: str) -> bool:
        """Validate that the response body matches expected content for this path."""
        if len(body) < 5:
            return False

        body_lower = body.lower()
        body_stripped = body_lower.strip()
        is_html = body_stripped.startswith((
            "<!doctype", "<html", "<?xml",
        )) or "<html" in body_lower[:500]

        if "404" in body_lower[:200] and "not found" in body_lower[:200]:
            return False

        if path == ".git/HEAD":
            return "ref:" in body and not is_html
        if path == ".git/config":
            return "[core]" in body and not is_html
        if path == ".git/index":
            return body.startswith("DIRC") and not is_html
        if path == ".git/logs/HEAD" or path.startswith(".git/logs/"):
            return bool(SHA1_RE.search(body)) and not is_html
        if path.startswith(".git/refs/"):
            stripped = body.strip()
            return len(stripped) >= 40 and all(
                c in "0123456789abcdef\n" for c in stripped[:41]
            )
        if path == ".git/packed-refs":
            return (
                not is_html
                and ("# pack-refs" in body or body.strip()[:1] in ("#",))
                and len(body.strip()) > 10
            )
        if path == ".git/objects/info/packs":
            return "P pack-" in body and not is_html
        if path == ".git/description":
            return not is_html and len(body.strip()) < 500
        if path == ".git/COMMIT_EDITMSG":
            return not is_html and len(body.strip()) < 2000
        if path == ".git/FETCH_HEAD":
            return bool(SHA1_RE.search(body)) and not is_html
        if path == ".git/ORIG_HEAD":
            return bool(SHA1_RE.match(body.strip()[:40])) and not is_html
        if path == ".git/info/exclude":
            return not is_html and len(body.strip()) > 0
        if path == ".git/info/refs":
            return bool(SHA1_RE.search(body)) and not is_html
        if path == ".git/shallow":
            return bool(SHA1_RE.search(body)) and not is_html
        if path == ".git/objects/info/alternates":
            return not is_html and len(body.strip()) > 0
        if path == ".gitignore":
            if is_html:
                return False
            lines = body.strip().splitlines()
            return any(
                ln.startswith(("#", "*", "/", "!")) or "=" not in ln
                for ln in lines[:10] if ln.strip()
            )
        if path == ".gitattributes":
            return not is_html and len(body.strip()) > 0
        if path.startswith(".env"):
            if is_html:
                return False
            return bool(re.search(r"^[A-Z_]+=", body, re.MULTILINE))
        if path == ".svn/entries":
            if is_html:
                return False
            first_line = body.strip().splitlines()[0] if body.strip() else ""
            return first_line.isdigit() or "<?xml" in body_lower[:50]
        if path == ".svn/wc.db":
            return body.startswith("SQLite format 3")
        if path == ".hg/dirstate":
            return not is_html and "\x00" in body[:50]
        if path == "CVS/Root":
            if is_html:
                return False
            return body.strip().startswith(":") or "/" in body.strip()[:20]
        if path == ".DS_Store":
            return not is_html and "\x00" in body[:20]
        if path.endswith((".php", ".php.bak")):
            if is_html:
                return False
            return "<?php" in body_lower[:100] or "<?" in body[:10]
        if path == "web.config":
            return "<?xml" in body_lower and "configuration" in body_lower
        if path == ".htaccess":
            if is_html:
                return False
            return (
                "rewrite" in body_lower or "deny" in body_lower
                or "order" in body_lower or "allow" in body_lower
            )
        if path == "server-status":
            return "apache" in body_lower and "server" in body_lower
        if path == "elmah.axd":
            return not is_html and "elmah" in body_lower
        if path == "phpinfo.php":
            return "php version" in body_lower or "phpinfo()" in body_lower

        return not is_html

    @staticmethod
    def _classify_path(path: str) -> tuple[str, str]:
        """Return (severity, description) for a path."""
        classification = {
            ".env": ("critical", "Environment file exposed"),
            ".env.local": ("critical", "Local environment file exposed"),
            ".env.production": ("critical", "Production environment file exposed"),
            ".env.staging": ("high", "Staging environment file exposed"),
            ".env.development": ("high", "Development environment file exposed"),
            ".env.backup": ("critical", "Env backup exposed"),
            ".svn/entries": ("medium", "SVN repository exposed"),
            ".svn/wc.db": ("medium", "SVN database exposed"),
            ".hg/dirstate": ("medium", "Mercurial repository exposed"),
            "CVS/Root": ("low", "CVS root exposed"),
            ".DS_Store": ("low", "macOS .DS_Store exposed"),
            "wp-config.php.bak": ("critical", "WordPress config backup exposed"),
            "web.config": ("high", "IIS web.config exposed"),
            ".htaccess": ("medium", "Apache .htaccess exposed"),
            "phpinfo.php": ("medium", "phpinfo() exposed"),
            "server-status": ("medium", "Apache server-status exposed"),
            "elmah.axd": ("medium", "ELMAH error log exposed"),
        }
        if path in classification:
            return classification[path]
        if path.startswith(".env"):
            return ("high", f"Environment file exposed: {path}")
        if path.startswith(".git/"):
            return ("high", f"Git file exposed: {path}")
        return ("low", f"Sensitive file exposed: {path}")
